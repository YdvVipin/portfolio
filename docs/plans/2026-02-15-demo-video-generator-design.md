# Demo Video Generator — Design Document

**Date:** 2026-02-15
**Status:** Approved

## Goal

Add short AI-narrated demo videos to portfolio project pages. Videos are generated by a Python pipeline that combines project screenshots/images with OpenAI TTS narration and animated transitions. This showcases AI-powered tooling skills to prospective employers.

## Architecture

### Pipeline (`tools/demo-generator/`)

```
tools/demo-generator/
├── generate.py          # Main CLI entry point
├── capture.py           # Playwright screenshot capture utility
├── configs/             # One YAML config per project
├── narrations/          # Generated TTS audio (gitignored)
├── frames/              # Generated slide frames (gitignored)
├── screenshots/         # Captured project screenshots (gitignored)
├── requirements.txt     # moviepy, openai, Pillow, pyyaml, playwright
└── README.md
```

### Generation Flow

1. Read YAML config (narration script, image paths, code snippets, timings)
2. Call OpenAI TTS API (`tts-1` model) → save MP3 narration per slide
3. Generate slide frames using Pillow (text overlays, code highlighting, zoom/pan keyframes)
4. moviepy assembles frames + audio → MP4 at 720p
5. Output to `assets/videos/{project-name}-demo.mp4`

### YAML Config Format

```yaml
project: ai-enabled-qa
title: "QA Automation AI Enabler"
voice: "nova"
resolution: [1280, 720]
slides:
  - image: "screenshots/ai-enabled-qa/01-dashboard.png"
    narration: "Narration text for this slide."
    duration: 8
    effect: "zoom_in"
  - code_snippet: |
      @app.post("/api/generate-tests")
      async def generate_tests(spec: TestSpec):
          agent = TestGeneratorAgent()
          return await agent.run(spec)
    narration: "Each endpoint triggers a specialized agent."
    duration: 6
    effect: "typewriter"
```

### Slide Effects

- `zoom_in` — slow zoom into center of image
- `zoom_out` — start zoomed in, pull out
- `pan_left` / `pan_right` — horizontal pan across wide images
- `typewriter` — code appears character by character
- `fade` — simple crossfade between slides

## Screenshot Capture

### For running apps (QA AI Enabler, etc.)
- Playwright script launches app, navigates key pages, captures at 1280x720
- `python capture.py --project ai-enabled-qa --url http://localhost:8000`

### For non-running projects
- Extract visuals from existing project detail HTML pages
- Capture architecture diagrams, tech stack sections, feature breakdowns
- VS Code screenshots of key code files

### Output
```
tools/demo-generator/screenshots/{project-name}/
├── 01-dashboard.png
├── 02-architecture.png
├── 03-feature.png
└── ...
```

## Portfolio Integration

### Video Player (CSS/JS)
- Custom HTML5 `<video>` player: `.kc-video-player` class prefix
- Dark-themed controls matching portfolio's `--accent` color scheme
- Play/pause, volume, progress bar, fullscreen
- Poster frame displayed before play
- Lazy loading: video src set on first click
- Fallback: "Demo coming soon" if no video file exists

### Embedding
- Video player inserted at top of each project detail page (`projects/{name}/index.html`)
- Added to existing `css/style.css` and `js/script.js`

### Project Cards Badge
- Cards on `pages/projects.html` with available demos get a `▶ Demo` badge overlay
- Clicking navigates to project page

## Project Priority

### Tier 1 (Build first)
| Project | Style | Duration |
|---------|-------|----------|
| QA Automation AI Enabler | Animated slides from architecture/dashboard | 2-3 min |
| Multi-Agent Orchestration | Architecture walkthrough | 1.5-2 min |
| Playwright BDD Framework | Code + architecture | 1.5-2 min |

### Tier 2 (Next)
- RAG System (1-2 min)
- Jira QA Automation with Grok AI (1-1.5 min)

### Tier 3 (Later)
- Remaining projects as needed

## Tech Stack

- Python 3.10+
- OpenAI TTS API (`tts-1`, voice: `nova`)
- moviepy (video assembly)
- Pillow (frame generation, text overlays)
- Playwright (screenshot capture)
- ffmpeg (backend for moviepy)
- pyyaml (config parsing)

## What This Proves to Employers

The demo generator itself is a portfolio piece — it demonstrates:
- API integration (OpenAI TTS)
- Media processing pipeline (Pillow + moviepy + ffmpeg)
- Automation mindset (Playwright capture)
- Config-driven architecture (YAML → video)
- End-to-end AI-assisted development workflow
